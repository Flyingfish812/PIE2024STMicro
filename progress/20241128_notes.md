#### Notes 11.28

Adam

- Considérer 4 classes ABCD, convertir en class numérique
- Ajouter colonnes : Gower's score (6 colonnes) et similarité cosinus
- Modèle "arbre aléatoire" : erreur en valeur absolue, en comparant les valeurs numériques, mais heure de calcul très long
- Optimiser le calcul par GPU ? Oui
- Tester sur sous-échantillon ? Perdre de l'information, 3h de run ça passe
- Distribution Gaussien ? Oui, tous les comportement sont sous Gaussien, choix correcte théoriquement. On peut tester avec U et comparer le résultat.
- Enlever F ? Pas grave parce que 7 est négligeable
- Intervalle valeur ? Pas de réponse correcte. Répartir plus ? Tester plus de choix
- Des structures de réseau de neuron : Neural network architecture for similarity

Yuran : 

- One-hot encodding est valable pour traiter le non-numérique, pas suffisante
- On peut développer la méthode "Forêt aléatoire"

Data : 

- Forme de donnée est déjà optimale

- Encoding, supplier package : trouver une façon d'éviter ajouter les colonnes, mieux de savoir quel est le plus proche - généraliser

- Semi supervisé, mettre plus de sampling
- Oversampling A c'est pas bon

Benchmarking des différents méthodes

Répartir tâches

- Data : non plus scrapping, augmenter la taille. Produit cartésien, semi supervisé pour les data, package encoding
- Algo : Trouver un meilleur algo (tester les autres algo), booster architecture, spécialiser le modèle (ex. cacher un moitié de xref comme nouveau information, spécialiser l'algo avec nouveau data)